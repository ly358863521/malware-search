from googlesearch import search_news
from bs4 import BeautifulSoup
import requests
from soup import soup
import xlsxwriter
import pandas as pd
df = pd.read_excel('Malware_name.xlsx')
for i in range(10,63):
        row = 1
        other_url = set()
        j = 0
        file_name = "info/info_"+str(i)+".xlsx"
        workbook = xlsxwriter.Workbook(file_name)
        worksheet = workbook.add_worksheet('sheet1')

        worksheet.write(0, 0, 'malware_name')
        worksheet.write(0,1, 'url')
        worksheet.write(0,2, 'author')
        worksheet.write(0,3, 'date')
        worksheet.write(0,4,'source')
        worksheet.write(0,5,'tpe')
        readDir = "../url_filter/"+str(i)+".txt"
        malware_name = df[df['family'] ==i].values[:,0]
        f = open(readDir,"r")
        url_num = 0
        for url in f:
                url = url.strip('\n')
                print(i,url_num,url)
                url_num+=1
                try:
                        info = soup(url)
                        if info:
                                
                                worksheet.write_row(row,0,[','.join(str(a) for a in malware_name),url,info['author'],info['p_date'],info['source'],info['tpe']])
                                row +=1
                        else:
                                # other_url.add(url)
                                j +=1
                                print(j)
                except:
                        print('error')
        workbook.close()