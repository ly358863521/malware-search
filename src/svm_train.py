import pandas as pd
import numpy as np
import xlsxwriter
from cal_ent import calc_ent_grap
from sklearn import svm
import nltk
import pandas as pd
import joblib
from wordnet import wordnet
# from nltk.corpus import wordnet as wn
from wordnet import wordnet
from time import sleep
def spl(text):
    a = [',','\"','?','.']
    for i in a:
        text = text.replace(i,' ')
    return text.split()
# del stopwords
# Stopwords considered as noise in the text. Text may contain stop words such as is, am, are, this, a, an, the, etc.
# In NLTK for removing stopwords, you need to create a list of stopwords and filter out your list of tokens from these words.
from nltk.corpus import stopwords
def stopwords(tokenized_sent):
    stop_words=set(stopwords.words("english"))
    print(stop_words)
    filtered_sent=[]
    for w in tokenized_sent:
        if w not in stop_words:
            filtered_sent.append(w)
    # print("Tokenized Sentence:",tokenized_sent)
    # print("Filterd Sentence:",filtered_sent)
    return filtered_sent

# cal_ent and wordnet Expansion
def ent_getword(text,rel):
    word_ent = {}
    word_list =set()
    for i in text:
        tokens = nltk.word_tokenize(text)
        filtered = stopwords(tokens)
        for j in filtered:
            if j not in word_list:
                word_list.add(j)
    for word in word_list:
        X = []
        for text_i in text:
            if text_i in text:
                X.append(1)
            else:
                X.append(0)
        word_ent[word] = calc_ent_grap(X,rel)
    word_ent = sorted(word_ent.items(),key = lambda d:d[1])[:20]
    word_list = set()
    for word in word_ent:
        for i in wordnet(word):   # wordnet  Expansion
            if i not in word_list:
                word_list.add(i)
    return word_list



kf = pd.read_excel('Malware_name.xlsx')
malware_name = kf.values[:,0]
df = pd.read_excel('entropy.xlsx')
data = df.head(200)
name = data.values[:,0]
other_name = data.values[:,1]
text = data.values[:,2]
rel = data.values[:,3]
rel = rel.astype('int')
# words = ent_getword(text,rel)
words = set(['new','same','evolution','likely','common','offspring','later','known','variant','beyond','faster','than','while','followed','like','variate','variance','from','version'])
words_temp = list(words)

def addlist(X,text,name,other_name):
    X.append([])
    tokens=nltk.word_tokenize(text)
    pos_tag = nltk.pos_tag(tokens)
    for word in words_temp:
        if word in tokens:
            X[-1].append(tokens.index(word))
            X[-1].append(pos_tag[tokens.index(word)][1])
        else:
            X[-1].append(0)
            X[-1].append('None')
    if name in tokens:
        X[-1].append(tokens.index(name))
        X[-1].append(pos_tag[tokens.index(name)][1])
    else:
        X[-1].append(0)
        X[-1].append('None')
    others = other_name.split(',')
    flag = True
    for other in others:
        if other in tokens:
            X[-1].append(tokens.index(other))
            X[-1].append(pos_tag[tokens.index(other)][1])
            flag = False
            break
    if flag:
        X[-1].append(0)
        X[-1].append('None')
    return X
    
def train_svm():
    X=[]
    for i in range(len(text)):
        X = addlist(X,text[i],name[i],other_name[i])
    X = pd.get_dummies(pd.DataFrame(X))
    print(X.shape,rel.shape)
    clf = svm.SVC(kernel='rbf',C=10,gamma='auto')
    clf.fit(X,rel)
    print(clf.score(X,rel))
    return clf,X.columns
def pred_svm(clf,columns):
    data = df.head(2000)
    name = data.values[:,0]
    other_name = data.values[:,1]
    text = data.values[:,2]
    rel = data.values[:,3]
    rel = rel.astype('int')
    relation = []
    file_name = 'relation_test.xlsx'
    workbook = xlsxwriter.Workbook(file_name)
    worksheet = workbook.add_worksheet('sheet1')
    worksheet.write(0, 0, 'name')
    worksheet.write(0,1, 'other_name')
    worksheet.write(0,2, 'relation')
    worksheet.write(0,3, 'text')
    row = 1
    for i in range(len(text)):
        print(i)
        X = []
        X = addlist(X,text[i],name[i],other_name[i])
        X = pd.get_dummies(pd.DataFrame(X))
        X = X.reindex(columns = columns,fill_value = 0)
        if clf.predict(X)[0] == 1:
            for k in other_name[i].split(','):
                for j in name[i].split(','):
                    if {k,j} not in relation:
                        relation.append({k,j})
                        worksheet.write_row(row,0,[k,j,1,text[i]])
                        row +=1
    workbook.close()

clf,columns =train_svm()
pred_svm(clf,columns)
# print(columns)
# X = np.array(X)
# print(X.shape,rel.shape)
# print(type(X[0][0]))
# print(X.columns)
# print(X.values[0])
# print(X[0])
# print(X.columns)
# joblib.dump(rf,'rf.model')



        


